# e1039-analysis/JobsubScript

A set of scripts in this directory is aimed at providing an convenient way to execute any Fun4All macro on GRID.
It is a generalized version of `SimChainDev/gridsub.sh`.

## Concept

You are supposed to have a Fun4All macro that runs fine with a single input; for example,
- `AnaRealDst/macro/Fun4RealDst.C`, which reads one real-data DST file, and
- `AnaSimDst/work/Fun4SimDst.C`, which reads one simulated DST file.
You will need to run it on GRID when the number of input files becomes large.
In such case you can use the scripts in this directory so as to
1. Set up and confirm that your macro runs fine on GRID,
1. Create and manage a list of job parameters, 
1. Submit a set of jobs of your macro,
1. Check if the output of each job is fine, and
1. Analyze all the outputs at once.

## Setting of ROOT Macro for GRID

Files accessible on GRID are quite limited.
Your ROOT macro has to use only
- Files under `/cvmfs/seaquest.opensciencegrid.org/seaquest/{software/e1039,users}` and
- Files that will be explicitly listed during the job submission.

You can use the two file categories above as you want, but usually
- Your own version of `e1039-core`, if you have, should go into the 1st category (via `/e906/app/software/osg`) and
- Other files (most likely of `e1039-analysis`) should be included in the 2nd category.

## Creation of Job-Parameter List

Each job has to be given a unique parameter set, which is for example
- An input file name in case of real-data anlaysis, and
- A serial number in case of simulation analysis.
You specify it, as many as you will submit, in the list file (`list_argument.txt`).
Note that the file name is selectable when you submit a job.
Each of lines in the list file is regarded as a parameter set per job.
The 1st parameter is used as "Job ID", which can contain only [a-zA-Z0-9].

`make_arg_list.sh` is a saple script to create the parameter list.

Simulation analysis might need just a serial number.
It can be generated by the `seq` command;
```
seq 1 100 >list_argument.txt
```

## Job Submission

You modify the three scripts below in order to run your ROOT macro properly;

### `config_common.sh`

This script configures shell parameters common to the other scripts.
You have to set at least the following parameters at the beginning;
- `JOB_NAME`
- `DIR_INPUT`
- `LIST_INPUT_FILES`

### `execute_multiple.sh`

This script reads the parameter-list file and submits a job of `execute_single.sh` per parameter set.
It simply passes a parameter set to `execute_single.sh` except one special case.
If a parameter starts with `/pnfs/` it is regarded as a file on pnfs and thus
- The file is prepared to be transfered at the beginning of job (via `-f` of `jobsub_submit`) and
- The file will be available under `$CONDOR_DIR_INPUT` in `execute_single.sh`.
Therefore *when you analyze (real or simulated) data file on pnfs you are recommended to include it in the parameter set*.

Actually this script does not submit any job by default but runs `execute_single.sh` locally for test.
```
./execute_single.sh -l 1-2
```
where `-l 1-2` means that only the 1st and 2nd lines of the parameter-list file are used (for test).
Note that this script skips a parameter set if the output directory for the parameter set already exists.
You can disable the skip by either using a `-o` option (`overwrite`) or manually deleting the output directory;
```
./execute_single.sh -l 1-2 -o
```

If the local test is successful, next you better try a test on GRID;
```
./execute_single.sh -g -l 1-5
```
Using this small GRID test,
you are recommended to confirm that the job outputs are reasonable and analyzable (via `check_output.sh` and `AnaOutput.C`).

Then you submit a large number of jobs to GRID;
```
./execute_single.sh -g -l 1-500
```
It is still better for you to use `-l` in order not to submit too many jobs at once.
Or you can create multiple list files in order to split job submissions, instead of using `-l`;
```
./execute_single.sh -g -f list_argument_028700.txt
```

### `execute_single.sh`

This script receives a set of parameters and execute your ROOT macro.
Probably you should modify this script so that
- The shell environment is properly set up for your macro and
- The parameter set is properly passed to your macro.

The handling of input/output files is limited since this script will be run on GRID.
All output files that you want to keep have to be moved into `$CONDOR_DIR_OUTPUT`.

## Check of Job Outputs

The status of your jobs can be checked by `jobsub_q_mine`.
After (almost) all jobs finish, you had better check if each job was successful.
You can do that just by running `check_output.sh`
```
./check_output.sh
```
which checks
1. The log file of each job exists?
1. Each log file contains the end message of the job script?
1. The ROOT file of each job exists?
1. Each ROOT file has a finite size?

## Analysis of Job Outputs

`AnaOutput.C` is a sample ROOT macro to analyze all job outputs, i.e. ROOT files.

## Author

Kenichi Naknao <knakano@nucl.phys.titech.ac.jp>
